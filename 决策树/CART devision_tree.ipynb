{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART分类决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义节点类，用以存放节点和节点特征\n",
    "class node:\n",
    "    def __init__(self, fea=-1, val=None, res=None, right=None, left=None):\n",
    "        self.fea = fea  # 特征\n",
    "        self.val = val  # 特征对应的值\n",
    "        self.res = res  # 叶节点标记\n",
    "        self.right = right # 定义左子树\n",
    "        self.left = left # 定义右子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个CART分类器\n",
    "class CART_classify:\n",
    "    #初始化\n",
    "    def __init__(self, epsilon=1e-3, min_sample=1):\n",
    "        self.epsilon = epsilon\n",
    "        self.min_sample = min_sample  # 叶节点含有的最少样本数\n",
    "        self.tree = None  #生成一棵空决策树\n",
    "        \n",
    "    #计算基尼系数函数定义\n",
    "    def Calculate_Gini(self, y_data):\n",
    "        # 计算基尼指数\n",
    "        dict_t = Counter(y_data) #统计标签类型及数量，形成标签字典\n",
    "        return 1 - sum([(val / y_data.shape[0]) ** 2 for val in dict_t.values()])\n",
    "    \n",
    "    #计算切分节点的基尼系数\n",
    "    def get_Feature_Gini(self, set1, set2):\n",
    "        # 计算某个特征及相应的某个特征值组成的切分节点的基尼指数\n",
    "        num = set1.shape[0] + set2.shape[0] #计算总的特征数量\n",
    "        return set1.shape[0] / num * self.Calculate_Gini(set2) + set2.shape[0] / num * self.Calculate_Gini(set2) #返回切分节点的基尼系数\n",
    "    \n",
    "    \n",
    "    \n",
    "    def bestSplit(self, splits_set, X_data, y_data):\n",
    "        # 返回所有切分点的基尼指数，以字典形式存储。键为split，是一个元组，第一个元素为最优切分特征，第二个为该特征对应的最优切分值\n",
    "        pre_gini = self.get_Feature_Gini(X_data, y_data)\n",
    "        subdata_inds = defaultdict(list)  # 切分点以及相应的样本点的索引\n",
    "        for split in splits_set:\n",
    "            for index, sample in enumerate(X_data): #enumerate函数生成一个枚举类型，包含X_data的元素及其下标，可以被遍历\n",
    "                if sample[split[0]] == split[1]:\n",
    "                    subdata_inds[split].append(index) #形成字典\n",
    "        min_gini = 1\n",
    "        best_split = None\n",
    "        best_set = None\n",
    "        for split, data_index in subdata_inds.items(): #循环遍历subdata_inds的键值对\n",
    "            set1 = y_data[data_index]  # 满足切分点的条件，则为左子树\n",
    "            set2_inds = list(set(range(y_data.shape[0])) - set(data_index))\n",
    "            set2 = y_data[set2_inds]\n",
    "            if set1.shape[0] < 1 or set2.shape[0] < 1:\n",
    "                continue\n",
    "            now_gini = self.get_Feature_Gini(set1, set2)\n",
    "            if now_gini < min_gini: #小于最小基尼系数，则更新最小基尼系数\n",
    "                min_gini = now_gini\n",
    "                best_split = split #同时更新最佳切分 \n",
    "                best_set = (data_index, set2_inds) #同时更新最佳叶节点\n",
    "        if abs(pre_gini - min_gini) < self.epsilon:  # 若切分后基尼指数下降未超过阈值则停止切分\n",
    "            best_split = None\n",
    "        return best_split, best_set, min_gini    \n",
    "    \n",
    "\n",
    "    def buildTree(self, splits_set, X_data, y_data):\n",
    "        if y_data.shape[0] < self.min_sample:  # 数据集小于阈值(只有)直接设为叶节点\n",
    "            return node(res=Counter(y_data).most_common(1)[0][0])\n",
    "        best_split, best_set, min_gini = self.bestSplit(splits_set, X_data, y_data)\n",
    "        if best_split is None:  # 基尼指数下降小于阈值，则终止切分，设为叶节点\n",
    "            return node(res=Counter(y_data).most_common(1)[0][0]) #返回标签数最多的一类作为该叶节点的标识\n",
    "        else:  #若基尼系数未小于阈值，则递归改函数，直至小于阈值\n",
    "            splits_set.remove(best_split)\n",
    "            left = self.buildTree(splits_set, X_data[best_set[0]], y_data[best_set[0]])\n",
    "            right = self.buildTree(splits_set, X_data[best_set[1]], y_data[best_set[1]])\n",
    "            return node(fea=best_split[0], val=best_split[1], right=right, left=left) #修改左右子树\n",
    "        \n",
    "    #训练函数\n",
    "    def train(self, X_data, y_data):\n",
    "        # 训练模型，CART分类树与ID3最大的不同是，CART建立的是二叉树，每个节点是特征及其对应的某个值组成的元组\n",
    "        # 特征可以多次使用\n",
    "        splits_set = [] #待切分为空\n",
    "        for fea in range(X_data.shape[1]):   #索引特征矩阵的每一列\n",
    "            unique_vals = np.unique(X_data[:, fea]) #将每一列（特征列）解析成列表\n",
    "            if unique_vals.shape[0] < 2: #如果该类特征值个数小于两个，则无需切分\n",
    "                continue\n",
    "            elif unique_vals.shape[0] == 2:  # 若该类特征值个数只有2个，则只有一个切分点，非此即彼\n",
    "                splits_set.append((fea, unique_vals[0])) #将特征标签和特征值以元组的形式添加进待切分列表\n",
    "            else:    # 若特征取值大于2个\n",
    "                for val in unique_vals:\n",
    "                    splits_set.append((fea, val)) #将特征标签和特征值以元组的形式添加进待切分列表\n",
    "        self.tree = self.buildTree(splits_set, X_data, y_data) #构建决策树\n",
    "        return\n",
    "    \n",
    "    def predict(self, x):\n",
    "        def helper(x, tree):\n",
    "            if tree.res is not None:  # 表明到达叶节点\n",
    "                return tree.res\n",
    "            else:\n",
    "                if x[tree.fea] == tree.val:  # \"是\" 返回左子树\n",
    "                    branch = tree.left\n",
    "                else:\n",
    "                    branch = tree.right\n",
    "                return helper(x, branch)\n",
    "\n",
    "        return helper(x, self.tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #加载sklearn中的鸢尾花数据集\n",
    "    X_data = load_iris().data \n",
    "    y_data = load_iris().target\n",
    "\n",
    "    #划分训练集和测试集\n",
    "    X_data_train, X_data_test, y_data_train, y_data_test = train_test_split(X_data, y_data, test_size=0.3, random_state=1)\n",
    "    classifier = CART_classify() #实例化一个分类器\n",
    "    classifier.train(X_data_train, y_data_train) #训练分类器\n",
    "    score = 0\n",
    "    for X, y in zip(X_data_test,y_data_test):\n",
    "        if classifier.predict(X) == y: #计算精度\n",
    "            score += 1 #预测正确则加1\n",
    "    print('accuracy is {}'.format(score / len(y_data_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测类型为第2类\n"
     ]
    }
   ],
   "source": [
    "## 小测试\n",
    "test = [1.5, 2.9, 4.6, 6.6]\n",
    "print('预测类型为第{}类'.format(classifier.predict(test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
